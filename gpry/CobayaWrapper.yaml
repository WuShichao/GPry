# Options regarding the bayesian optimization loop.
# NB: 'd' after a number means the dimensionality of the sampling space,
#     and a number following 'd' means the power of the dimensionality factor.

# General options for the main loop
options:
  # Number of finite initial truth evaluations before starting the learning loop
  n_initial: 3d
  # Maximum number of truth evaluations at initialization. If it is reached before
  # `n_initial` finite points have been found, the run will fail. To avoid that, try
  # decreasing the volume of your prior
  max_initial: 30d1.5
  # Maximum number of truth evaluations before the run stops. This is useful for e.g.
  # restricting the maximum computation resources.
  max_total: 70d1.5
  # Maximum number of sampling points accepted into the GP training set before the run
  # stops. If this limit is frequently saturated, try decreasing the prior volume.
  max_finite:  # default (undefined) = max_total
  # Number of points which are aquired with Kriging believer for every acquisition step.
  # Gets adjusted (with 20% tol.) to match a multiple of the num. of parallel processes.
  n_points_per_acq: d
  # Number of iterations between full GP hyperparameters fits, including several
  # restarts of the optimiser. Pass 'np.inf' or a large number to never refit with restarts.
  fit_full_every: 2d0.5
  # Similar to `fit_full_every`, but with a single optimiser run from the last optimum.
  # Overridden by `fit_full_every` if it applies. Pass np.inf or a large number to never
  # refit from last optimum (hyperparameters kept constant in that iteration).
  fit_simple_every: 1  # every iteration

# The GP regressor used for interpolating the posterior, with options
gpr:
  kernel: RBF  # e.g. RBF, Matern, {Matern: {nu: 2.5}}, ...
  noise_level: 1e-2
  optimizer: fmin_l_bfgs_b
  n_restarts_optimizer:  # default (undefined) is 10 + 2 * dim
  account_for_inf: SVM  # Undefined or null for not using an SVM classificator
  verbose:  # set only if you want different verbosity for the GPR

# Acquisition class. If simply passed the name of an acquisition function,
# (e.g. "LogExp") uses the default BatchOptimizer with that function.
gp_acquisition:
  BatchOptimizer:
    acq_func:
      LogExp: {zeta_scaling: 0.85}
    proposer:  # default (undefined): a mixture of uniform and centroids
    acq_optimizer: fmin_l_bfgs_b
    n_restarts_optimizer: 5d
    n_repeats_propose: 10
    verbose:  # set only if you want different verbosity for the acq process
#  NORA:
#    acq_func:
#      LogExp: {zeta_scaling: 0.85}
#    mc_every: 2d
#    nlive_per_training: 3
#    nlive_per_dim_max: 25
#    num_repeats_per_dim: 5
#    precision_criterion_target: 0.005
#    nprior_per_nlive: 10
#    verbose:  # set only if you want different verbosity for the acq process

# Proposer used for drawing the initial training samples before running
# the acquisition loop. One of [reference, prior, uniform].
# Can be specified as dict with args, e.g. {reference: {max_tries: 1000}}
initial_proposer: reference

# Convergence criterion.
# Can be specified as a dict with args, e.g. {CorrectCounter: {abstol: 0.01s}}
convergence_criterion: CorrectCounter

# Cobaya sampler used to generate the final sample from the surrogate model
mc_sampler: mcmc  # default: mcmc with Cobaya defaults

# Produce progress plots (inside the gpry_output dir).
# Adds overhead for very fast likelihoods.
plots: False

# Function run each iteration after adapting the recently acquired points and
# the computation of the convergence criterion. See docs for implementation.
callback:

# Whether the callback function handles MPI-parallelization internally.
# Otherwise run only by the rank-0 process
callback_is_MPI_aware:

# Change to increase or reduce verbosity. If None, it is handled by Cobaya.
# '3' produces general progress output (default for Cobaya if None),
# and '4' debug-level output
verbose:
